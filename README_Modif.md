# üìù Journal des Modifications - Scientific Research Navigator

Ce document retrace toutes les modifications apport√©es au syst√®me, avec explications techniques et justifications.

---

## üìÖ 8 F√©vrier 2026

### üéØ D0: Audit d'Architecture

**Objectif**: Documenter l'√©tat actuel du syst√®me et planifier l'√©volution vers NotebookLM-like

#### Fichiers cr√©√©s
- **ARCHITECTURE.md** (500+ lignes)

#### Contenu
```
‚úì Audit complet des mod√®les Django
‚úì Inventaire des endpoints API existants
‚úì Documentation du pipeline RAG actuel
‚úì Identification des gaps techniques
‚úì Roadmap d√©taill√©e en 7 phases (D0-D7)
‚úì Timeline de 20 jours avec estimations
```

#### Justification
- Besoin de comprendre la base de code avant d'ajouter des fonctionnalit√©s complexes
- Documentation n√©cessaire pour les futurs d√©veloppeurs
- Plan structur√© pour √©viter la dette technique

---

### üéØ D1: Pipeline d'Ingestion Unifi√© Asynchrone

**Objectif**: Remplacer l'upload synchrone bloquant (30-60s timeout) par un syst√®me asynchrone avec suivi d'√©tat

#### üîß Modifications Backend

##### 1. **Mod√®le Document** (`backend/rag/models.py`)
**Ajout de 4 nouveaux champs**:
```python
status = models.CharField(
    max_length=20,
    choices=[
        ('UPLOADED', 'Uploaded'),
        ('PROCESSING', 'Processing'),
        ('INDEXED', 'Indexed'),
        ('FAILED', 'Failed'),
    ],
    default='UPLOADED'
)
processing_started_at = models.DateTimeField(null=True, blank=True)
processing_completed_at = models.DateTimeField(null=True, blank=True)
error_message = models.TextField(null=True, blank=True)
```

**Pourquoi ?**
- Tracking pr√©cis de l'√©tat de chaque document
- Permet au frontend de poller l'√©tat sans bloquer
- Capture des erreurs pour debugging
- M√©triques de performance (temps de traitement)

##### 2. **Service d'Ingestion** (`backend/rag/services/ingestion.py`)
**Nouvelle classe**: `IngestionService`

**M√©thodes principales**:
```python
def ingest_document(self, document_id: int, pdf_path: str) -> bool:
    """
    Pipeline complet d'ingestion avec:
    - Logging d√©taill√© √† chaque √©tape
    - Gestion d'erreurs robuste (try/except)
    - Mise √† jour automatique des status
    - Extraction m√©tadonn√©es + chunking + indexation Chroma
    """

def reingest_document(self, document_id: int, pdf_path: str) -> bool:
    """
    Retry pour documents FAILED:
    - Reset du status √† UPLOADED
    - Nettoyage des anciennes erreurs
    - Relance de l'ingestion compl√®te
    """
```

**Avantages**:
- S√©paration des responsabilit√©s (SRP)
- Code testable isol√©ment
- R√©utilisable (upload manuel vs arXiv import vs PubMed)
- Logs centralis√©s

##### 3. **Vue Upload Asynchrone** (`backend/rag/views.py`)
**Avant** (bloquant):
```python
def upload_pdf(request):
    # Sauvegarde du fichier
    # Ingestion synchrone (30-60s) ‚ùå
    ingest.ingest_pdf(...)
    return Response(status=201)  # Apr√®s 60s
```

**Apr√®s** (non-bloquant):
```python
def upload_pdf(request):
    # 1. Sauvegarde du fichier
    # 2. Cr√©ation du Document avec status=UPLOADED
    
    # 3. Lancement du thread background
    def ingest_in_background():
        service = IngestionService()
        service.ingest_document(document.id, str(full_path))
    
    thread = threading.Thread(target=ingest_in_background, daemon=True)
    thread.start()
    
    # 4. Retour IMMEDIAT (< 100ms)
    return Response({
        "message": "PDF upload initiated. Processing in background.",
        "document_id": document.id,
        "status": "UPLOADED"
    }, status=202)  # 202 Accepted ‚úÖ
```

**B√©n√©fices**:
- UX am√©lior√©e: pas de timeout frontend
- Scalabilit√©: peut traiter plusieurs uploads simultan√©ment
- Robustesse: √©chec d'un document n'affecte pas les autres
- RESTful: 202 Accepted = "requ√™te accept√©e, traitement asynchrone"

##### 4. **Endpoint de Status** (`backend/rag/views.py` + `urls.py`)
**Nouveau endpoint**: `GET /api/documents/<id>/status/`

**R√©ponse**:
```json
{
  "document_id": 1,
  "filename": "paper.pdf",
  "session": "yahia",
  "status": "INDEXED",
  "uploaded_at": "2026-02-08T20:13:57Z",
  "processing_started_at": "2026-02-08T20:13:57Z",
  "processing_completed_at": "2026-02-08T20:13:58Z",
  "processing_time_seconds": 1.55,
  "error_message": null,
  "metadata": {
    "title": "...",
    "abstract": "...",
    "page_count": 14
  }
}
```

**Usage**:
```javascript
// Frontend polling pattern
async function pollStatus(documentId) {
  while (true) {
    const response = await fetch(`/api/documents/${documentId}/status/`);
    const data = await response.json();
    
    if (data.status === 'INDEXED') {
      showSuccess('Document ready for queries!');
      break;
    } else if (data.status === 'FAILED') {
      showError(data.error_message);
      break;
    }
    
    await sleep(2000); // Poll toutes les 2s
  }
}
```

##### 5. **Migration Base de Donn√©es**
**Fichier**: `backend/rag/migrations/0006_document_error_message_and_more.py`

**Op√©rations**:
```python
operations = [
    migrations.AddField(
        model_name='document',
        name='error_message',
        field=models.TextField(blank=True, null=True),
    ),
    migrations.AddField(
        model_name='document',
        name='processing_completed_at',
        field=models.DateTimeField(blank=True, null=True),
    ),
    migrations.AddField(
        model_name='document',
        name='processing_started_at',
        field=models.DateTimeField(blank=True, null=True),
    ),
    migrations.AddField(
        model_name='document',
        name='status',
        field=models.CharField(
            choices=[...],
            default='UPLOADED',
            max_length=20
        ),
    ),
]
```

**Appliqu√©e avec**: `python manage.py migrate`

#### ‚úÖ Tests Unitaires

**Fichier**: `backend/rag/tests/test_ingestion.py`

**8 tests impl√©ment√©s**:

1. **`test_successful_ingestion`**: V√©rifie que l'ingestion r√©ussie met status=INDEXED
2. **`test_ingestion_failure`**: V√©rifie que les exceptions sont captur√©es (status=FAILED)
3. **`test_ingest_nonexistent_document`**: V√©rifie rejet des IDs invalides
4. **`test_upload_returns_202`**: V√©rifie r√©ponse HTTP correcte (202 Accepted)
5. **`test_upload_no_file`**: V√©rifie validation (erreur 400 si pas de fichier)
6. **`test_upload_non_pdf`**: V√©rifie validation (erreur 400 si pas un PDF)
7. **`test_get_document_status`**: V√©rifie format de r√©ponse du status endpoint
8. **`test_get_document_status_nonexistent`**: V√©rifie gestion 404 pour IDs invalides

**R√©sultats**: ‚úÖ 8/8 PASSED (0.145s)

**Commande**: `python manage.py test rag.tests.test_ingestion --no-input`

#### üìö Documentation

##### README.md
**Ajouts**:
- Section "Asynchronous document processing" dans Features
- Documentation compl√®te de l'API avec exemples curl
- Workflow de polling expliqu√©
- Exemples de r√©ponses JSON

##### ARCHITECTURE.md
- R√©f√©rence comme documentation de base pour comprendre le syst√®me

#### üß™ Smoke Tests Valid√©s

```bash
# 1. Sessions endpoint
curl http://localhost:8000/api/sessions/
‚úì Retourne liste des sessions

# 2. Liste PDFs avec status
curl http://localhost:8000/api/pdfs/?session=yahia
‚úì Retourne documents avec champ "status"

# 3. Upload asynchrone
curl -X POST http://localhost:8000/api/upload/ \
  -F "file=@paper.pdf" \
  -F "session=yahia"
‚úì Retourne 202 Accepted avec document_id

# 4. Status polling
curl http://localhost:8000/api/documents/1/status/
‚úì Retourne status d√©taill√© avec processing_time_seconds

# 5. V√©rification transition d'√©tat
# Apr√®s 2 secondes: status passe de UPLOADED ‚Üí INDEXED
‚úì Processing time mesur√©: 1.55 secondes
```

---

## üìä M√©triques D1

| M√©trique | Valeur |
|----------|--------|
| Lignes de code ajout√©es | 1,232 |
| Fichiers modifi√©s | 10 |
| Tests cr√©√©s | 8 |
| Taux de r√©ussite tests | 100% |
| Temps de traitement mesur√© | 1.55s |
| Temps r√©ponse upload | < 100ms (vs 30-60s avant) |

---

## üéØ Impact Business

### Avant D1 ‚ùå
- Upload bloquant 30-60 secondes
- Timeout frontend si PDF volumineux
- Pas de feedback pendant le traitement
- Impossible de savoir si l'indexation a √©chou√©
- Un √©chec bloque toute l'application

### Apr√®s D1 ‚úÖ
- Upload instantan√© (< 100ms)
- Feedback temps r√©el avec polling
- Tra√ßabilit√© compl√®te (timestamps, dur√©e, erreurs)
- Ingestions parall√®les possibles
- √âchecs isol√©s et "retryables"
- Ready pour int√©gration arXiv/PubMed (D2/D3)

---

## üîó Git

**Branch**: `feature/unified-ingestion`
**Commit**: `cab1842` - "feat(D1): Unified asynchronous ingestion pipeline"
**Push**: ‚úÖ Pouss√© sur GitHub
**PR**: https://github.com/yzriga/PFE_AI/pull/new/feature/unified-ingestion

---

## üöÄ Prochaines √âtapes

### En Attente
- [ ] Merger feature/unified-ingestion ‚Üí main
- [ ] D√©marrer D2: arXiv Connector

### D2 Pr√©vu (arXiv Connector)
**Scope**:
- Service `ArxivService` (search + download)
- Mod√®le `PaperSource` (arXiv ID, DOI, metadata)
- Endpoints: `GET /api/arxiv/search`, `POST /api/arxiv/import`
- Tests avec mocks arXiv API
- Frontend: Composant recherche arXiv

**Estimation**: 3-4 heures

---

## üìù Notes Techniques

### Choix d'Architecture

**Threading vs Celery**:
- ‚úÖ Threading choisi pour D1 (simplicit√©, pas de d√©pendances)
- ‚ö†Ô∏è Celery recommand√© pour production (D6: Monitoring)
- Raison: Threading suffit pour MVP, Celery ajout√© plus tard

**Status Fields vs √âtat Machine**:
- ‚úÖ CharField avec choices (simple, queryable)
- Alternative consid√©r√©e: django-fsm (overkill pour 4 √©tats)

**Polling vs WebSocket**:
- ‚úÖ Polling REST (compatible avec infrastructure actuelle)
- WebSocket envisag√© pour D7 (frontend redesign)

### Lessons Learned

1. **Tests d'abord**: Les tests mock ont r√©v√©l√© un bug de threading avant production
2. **Logging essentiel**: Chaque √©tape logg√©e = debugging 10x plus rapide
3. **Migration test√©e**: Toujours tester migrate sur copie DB avant production
4. **Documentation synchronis√©e**: README mis √† jour AVANT le push (pas apr√®s)

---

*Derni√®re mise √† jour: 8 f√©vrier 2026 - D1 compl√©t√©*
